# HCache 开发日志 (一)：初期架构与核心设计

## 缘起

回想起设计 HCache 的契机，是源于一次生产环境的性能瓶颈分析。那是一个处理金融交易的微服务系统，在高峰期，我们发现数据库成为了明显的性能瓶颈。引入缓存是必然选择，但市场上的缓存方案各有所长，也各有不足：

- Redis 作为远程缓存需要网络开销，且序列化/反序列化成本不可忽视
- 现有的本地缓存库要么功能单一，要么性能表现不尽如人意
- 大多数方案在高并发场景下存在锁竞争问题
- 缺乏对内存压力的精细控制，容易引发 OOM

这让我开始思考：如何设计一个更符合现代 Go 服务需求的内存缓存库？它应当足够高效，提供丰富的功能，同时避免现有方案的痛点。

## 设计理念

HCache 的设计基于几个核心理念：

1. **性能优先**：缓存的首要目标是提升系统性能，因此 HCache 的每一个设计决策都首先考虑性能影响
2. **分片架构**：通过数据分片减少锁竞争，充分利用多核优势
3. **内存可控**：精确控制内存使用，防止缓存导致的 OOM 问题
4. **灵活配置**：提供丰富的配置选项，适应不同的使用场景
5. **接口优雅**：简洁且强大的 API 设计，降低使用门槛

## 核心架构设计

### 分层设计

经过反复推敲，我决定采用清晰的分层架构：

```
┌─────────────────────────────────────┐
│            Public API               │
│  (易用性、类型安全、功能完备)          │
├─────────────────────────────────────┤
│           Internal Core             │
│  (高性能、并发安全、资源管理)          │
└─────────────────────────────────────┘
```

这种分层既能保证 API 的稳定性，又能使内部实现灵活调整。初期设计决策包括：

1. 公开层使用接口定义，隐藏实现细节
2. 内部层关注并发控制和性能优化
3. 将功能模块化，实现可插拔设计

### 分片机制设计

分片是 HCache 应对高并发的核心策略。设计之初，我考虑了两种分片方案：

1. **静态分片**：预先分配固定数量的分片
2. **动态分片**：根据负载动态调整分片数

经过原型测试，我发现静态分片在大多数场景下已经能提供足够好的性能，同时避免了动态分片带来的复杂性和额外开销。分片数量默认设置为 16，这是对大多数服务器 CPU 核心数的平衡考虑。

```go
// 分片设计的核心思路
type ShardedCache struct {
    shards    []*cacheShard
    shardMask uint64
    hash      fnv64a
}

func (c *ShardedCache) getShard(key string) *cacheShard {
    hashedKey := c.hash.Sum64(key)
    return c.shards[hashedKey&c.shardMask]
}
```

通过位掩码操作而非取模运算来确定分片索引，这是一个微小但重要的优化，在高频调用时可以显著提升性能。

### 并发控制策略

缓存操作的并发控制至关重要。我选择了精细化的锁策略，而非全局锁或无锁设计：

1. **每个分片一把锁**：细粒度锁控制，减少锁竞争
2. **读写分离**：对于读多写少的场景，考虑读写锁而非互斥锁
3. **原子操作**：对于计数器等简单操作，使用原子操作而非锁

初期测试表明，这种并发控制策略在 8-32 核的服务器上表现最为出色，能够实现近乎线性的扩展。

## 首个难题：内存管理

设计过程中遇到的第一个真正难题是内存管理。Go 的 GC 机制对于缓存这类大内存应用并不总是友好，尤其是在处理大量小对象时。

我尝试了几种策略：

1. **直接存储对象引用**：实现简单，但会增加 GC 压力
2. **序列化为字节数组**：减轻 GC 压力，但增加 CPU 开销
3. **对象池化**：减少内存分配，但增加代码复杂度

最终决定采用混合策略：

```go
type cacheEntry struct {
    key        string
    value      interface{}   // 小对象直接存储
    data       []byte        // 大对象序列化存储
    expiration int64
    size       int64
    // 其他元数据...
}
```

通过可配置的阈值，决定对象是直接存储还是序列化存储。这是对 CPU 和内存压力的平衡考虑。

## 首个版本的局限性

第一个版本实现后，我进行了广泛的基准测试，发现了几个值得注意的局限性：

1. **内存估算不精确**：Go 的内存模型使得精确计算对象大小变得困难
2. **GC 压力问题**：大量缓存条目仍会给 GC 带来压力
3. **过期清理策略不够高效**：定时扫描带来的性能抖动
4. **单一淘汰策略**：初版仅实现了 LRU，无法适应所有访问模式

这些问题成为了下一阶段优化的重点方向。特别是 GC 压力问题，这是 Go 语言内存缓存实现的普遍挑战，需要更精巧的设计来解决。

## 反思与展望

回顾这个阶段的工作，我对架构的基本决策比较满意，尤其是分片设计和接口抽象。但也意识到，一个真正高性能的缓存库需要在更多细节上精益求精：

1. 需要更精确的内存管理机制
2. 需要更高效的过期策略
3. 需要更多样的淘汰算法
4. 需要更完善的指标收集

下一阶段的重点将是解决内存管理和 GC 压力问题，这是构建高性能 Go 缓存库的核心挑战。我计划研究 bigcache 和 freecache 等库的实现，寻找更优的内存管理策略。

最后，值得一提的是，这个初始版本已经在我们的内部服务中得到了应用，相比之前使用的缓存方案，性能提升了约 30%，内存使用减少了约 20%。这给了我继续优化的信心和动力。

*"架构的本质不是设计，而是决策和平衡。"* 